{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Literal, BNode, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD, DC\n",
    "from Levenshtein import distance as edit_distance\n",
    "from tqdm import tqdm\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from collections import defaultdict\n",
    "import tomllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().resolve() / \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser import parse_and_extract_articles_langs_from_dirs\n",
    "\n",
    "ROOT = Path().resolve()\n",
    "config_path =  \"config.toml\"\n",
    "\n",
    "with open(config_path, \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "rdf_dirs = [ROOT / Path(d) for d in config[\"paths\"][\"rdf_dirs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 RDF files in /Users/vlermanda/Main/Internship/data/articles/resourcedump_000130/rdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f458f92fbe2464194d0a14a1d10d535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 15898 English and 18265 Japanese articles; skipped 0 empty files.\n",
      "Found 50000 RDF files in /Users/vlermanda/Main/Internship/data/articles/resourcedump_000700/rdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d19b16c75bc425ba8f388a9c0549a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://cir.nii.ac.jp/all?q=GABA_{A}%20receptor does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 11929 English and 48252 Japanese articles; skipped 0 empty files.\n",
      "Found 50000 RDF files in /Users/vlermanda/Main/Internship/data/articles/resourcedump_001367/rdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aaefec8e3e443e5a0474231a8c70091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://cir.nii.ac.jp/all?q=%3CSUP%3E13%3C/SUP%3EC%E2%80%93{<SUP>1</SUP>H}%20Noise%20Decoupling does not look like a valid URI, trying to serialize this will break.\n",
      "https://cir.nii.ac.jp/all?q=%3CSUP%3E13%3C/SUP%3EC%E2%80%93{<SUP>1</SUP>H}%20NMR does not look like a valid URI, trying to serialize this will break.\n",
      "https://cir.nii.ac.jp/all?q=Poly{3-[3-(4-hydroxyphenyl)phthalidyl]-4-hydroxystyrene-<I>co</I>-4-hydroxystyrene} does not look like a valid URI, trying to serialize this will break.\n",
      "https://cir.nii.ac.jp/all?q=%3CSUP%3E13%3C/SUP%3EC%E2%80%93{<SUP>1</SUP>H}%20NMR does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 11757 English and 10062 Japanese articles; skipped 37296 empty files.\n"
     ]
    }
   ],
   "source": [
    "en_articles, jp_articles = parse_and_extract_articles_langs_from_dirs(rdf_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrames\n",
    "df_en = pd.DataFrame.from_dict(en_articles, orient='index').reset_index(drop=True)\n",
    "df_jp = pd.DataFrame.from_dict(jp_articles, orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = ROOT / \"data\" / \"cache\"\n",
    "\n",
    "df_en.to_pickle(cache_dir / \"df_en.pkl\")\n",
    "df_jp.to_pickle(cache_dir / \"df_jp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed entries: 549\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match Japanese characters (Hiragana, Katakana, Kanji)\n",
    "jp_pattern = re.compile(r'[\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff]')\n",
    "\n",
    "# Find rows with Japanese characters in the title\n",
    "jp_mask = df_en['title'].apply(lambda x: bool(jp_pattern.search(str(x))))\n",
    "num_removed = jp_mask.sum()\n",
    "\n",
    "# Remove those rows\n",
    "df_en = df_en[~jp_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of removed entries: {num_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed Spanish/French entries (including accented): 49\n"
     ]
    }
   ],
   "source": [
    "# Define basic regex patterns for Spanish and French detection\n",
    "spanish_pattern = re.compile(r'\\b(el|la|los|las|un|una|unos|unas|de|y|en|que|por|para|con|sin|del|al|se|su|sus|es|son|como|pero|más|o|le|lo|mi|tu|su|nos|vos|ellos|ellas|este|esta|estos|estas|ese|esa|esos|esas|aquel|aquella|aquellos|aquellas)\\b', re.IGNORECASE)\n",
    "french_pattern = re.compile(r'\\b(le|la|les|un|une|des|du|de|et|en|que|pour|avec|sans|dans|sur|par|au|aux|ce|cette|ces|son|sa|ses|est|sont|comme|mais|plus|ou|mon|ton|notre|votre|leur|leurs|il|elle|ils|elles|cet|cette|ceux|celles)\\b', re.IGNORECASE)\n",
    "\n",
    "accented_pattern = re.compile(r'[áéíóúüñçàèìòùâêîôûëïüœæ]', re.IGNORECASE)\n",
    "\n",
    "# Remove rows with Spanish/French stopwords or accented characters in the title or abstract\n",
    "es_fr_mask = (\n",
    "    df_en['title'].apply(lambda x: bool(spanish_pattern.search(str(x))) or\n",
    "                                   bool(french_pattern.search(str(x))) or\n",
    "                                   bool(accented_pattern.search(str(x))))\n",
    "    | df_en['abstract'].apply(lambda x: bool(spanish_pattern.search(str(x))) or\n",
    "                                         bool(french_pattern.search(str(x))) or\n",
    "                                         bool(accented_pattern.search(str(x))))\n",
    ")\n",
    "num_es_fr_removed = es_fr_mask.sum()\n",
    "df_en = df_en[~es_fr_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of removed Spanish/French entries (including accented): {num_es_fr_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of characters: 334.53\n",
      "Average number of words: 50.47\n"
     ]
    }
   ],
   "source": [
    "avg_char_length = df_en['abstract'].apply(len).mean()\n",
    "avg_word_length = df_en['abstract'].apply(lambda x: len(x.split())).mean()\n",
    "\n",
    "print(f\"Average number of characters: {avg_char_length:.2f}\")\n",
    "print(f\"Average number of words: {avg_word_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 36484\n",
      "Mean cluster size: 1.02\n",
      "Median cluster size: 1.0\n",
      "Max cluster size: 35\n",
      "Min cluster size: 1\n",
      "Number of clusters with size > 10: 5\n"
     ]
    }
   ],
   "source": [
    "# Cluster Sizes \n",
    "cluster_sizes = [len(docs) for docs in clusters.values()]\n",
    "print(f\"Number of clusters: {len(cluster_sizes)}\")\n",
    "\n",
    "if cluster_sizes:\n",
    "\tprint(f\"Mean cluster size: {np.mean(cluster_sizes):.2f}\")\n",
    "\tprint(f\"Median cluster size: {np.median(cluster_sizes)}\")\n",
    "\tprint(f\"Max cluster size: {np.max(cluster_sizes)}\")\n",
    "\tprint(f\"Min cluster size: {np.min(cluster_sizes)}\")\n",
    "\tprint(f\"Number of clusters with size > 10: {sum(size > 10 for size in cluster_sizes)}\")\n",
    "else:\n",
    "\tprint(\"No clusters found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords: 100%|██████████| 39035/39035 [42:56<00:00, 15.15it/s]   \n"
     ]
    }
   ],
   "source": [
    "from preprocess import extract_keywords_df\n",
    "\n",
    "# Extract keywords from the DataFrame\n",
    "df_en_kw, topics = extract_keywords_df(\n",
    "    df_en,\n",
    "    text_cols=(\"title\", \"abstract\"),\n",
    "    top_k=8,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    "    keywords_col=\"keywords\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(topics, columns=[\"phrase\", \"frequency\"])\n",
    "topics_df.to_csv(\"extracted_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 topics:\n",
      "                phrase  frequency\n",
      "             education        174\n",
      "               history        173\n",
      "antibacterial activity        156\n",
      "                   law        154\n",
      "                 japan        142\n",
      "              japanese        123\n",
      "         antibacterial        120\n",
      "     cephem antibiotic        115\n",
      "                   art        108\n",
      "                  book        108\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 topics\n",
    "print(\"Top 10 topics:\")\n",
    "print(topics_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_treshold = 5\n",
    "vocab = {kw for kw, df in topics if df >= freq_treshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import csv, time\n",
    "\n",
    "top_level_topics = [\n",
    "    \"Natural Sciences\",\n",
    "    \"Engineering and Technology\",\n",
    "    \"Medical and Health Sciences\",\n",
    "    \"Agricultural Sciences\",\n",
    "    \"Social Sciences\",\n",
    "    \"Arts and Humanities\",\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "For each keyword decide which TOP-LEVEL RESEARCH FIELD it belongs to.\n",
    "Return a CSV: keyword,field.\n",
    "Use exactly one of:\n",
    "{fields}\"\"\".format(fields=\" | \".join(top_level_topics))\n",
    "\n",
    "def map_keywords(keywords, model=\"gpt-4o-mini\", batch=100):\n",
    "    client   = OpenAI()\n",
    "    mapping  = {}\n",
    "\n",
    "    for start in tqdm(range(0, len(keywords), batch), desc=\"LLM-mapping\"):\n",
    "        chunk = keywords[start:start+batch]\n",
    "        user  = \"\\n\".join(chunk)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\",   \"content\": user},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        for row in csv.reader(response.choices[0].message.content.splitlines()):\n",
    "            if len(row) < 2: \n",
    "                continue\n",
    "            kw, field = row[0].strip(), row[1].strip()\n",
    "            if field in top_level_topics:\n",
    "                mapping[kw] = field\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM-mapping: 100%|██████████| 31/31 [07:42<00:00, 14.91s/it]\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib\n",
    "\n",
    "keyword_field = map_keywords(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         field  count\n",
      "1  Medical and Health Sciences   1003\n",
      "3              Social Sciences    992\n",
      "2          Arts and Humanities    455\n",
      "4   Engineering and Technology    249\n",
      "0             Natural Sciences    238\n",
      "5        Agricultural Sciences     87\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "field_counts = Counter(keyword_field.values())\n",
    "field_counts_df = pd.DataFrame(field_counts.items(), columns=[\"field\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "\n",
    "print(field_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'field' column to df_en_kw by mapping each paper's main topic to its field\n",
    "def get_main_field(keywords, keyword_field):\n",
    "    for kw in keywords:\n",
    "        if kw in keyword_field:\n",
    "            return keyword_field[kw]\n",
    "    return \"Error Name\"\n",
    "\n",
    "df_en_kw['field'] = df_en_kw['keywords'].apply(lambda kws: get_main_field(kws, keyword_field))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "internship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
